# LLM の強化学習手法 - 学習メモ (2025-12-08)

> 前提知識: 勾配降下法、Loss関数、Optimizer は知っている。RLは知らない。

## 背景

Nex-N1 などの論文で「RL (強化学習) で学習」と書いてあるが、具体的に何をしているのか理解したかった。

---

## 教師あり学習 vs 強化学習

| | 教師あり学習 | 強化学習 |
|---|---|---|
| 正解 | 明確に存在 | 不明確（複数ありうる） |
| 評価 | Loss (正解との差) | Reward (報酬) |
| 例 | 「1+1=2」が正解 | 「このコードは良いか？」 |

強化学習は「正解」の代わりに「報酬」で評価する。

---

## 最初の疑問: Reward Model を含んだ Loss は微分可能？

**答え: 微分しない**

### 問題

```
プロンプト → LLM → 確率分布 → [サンプリング] → トークン → [報酬モデル] → スコア
                              ↑
                        離散的選択（微分不可能）
```

### 解決: Policy Gradient 定理

数学的トリックで reward を微分の外に出す:

```
∇E[R] = E[ ∇log P(x) × R(x) ]
            ↑           ↑
         微分する    微分しない（ただの係数）
```

reward は勾配の「方向」を決める係数として使う。微分はしない。

---

## REINFORCE（基本手法）

```
勾配 = reward × ∇log P(x)
```

- reward > 0 (good) → その出力の確率を上げる
- reward < 0 (bad) → その出力の確率を下げる

**理解した表現**:
> good 報酬の場合は勾配に + な値、bad な値の場合は勾配に -1 をかけて学習。

---

## PPO（効率化手法）

### REINFORCE の問題

```
サンプル生成 → 更新 → モデルが変わる → 古いサンプルは使えない
```

毎回生成し直すのは高コスト。

### PPO の解決策

```
ratio = P_new(x) / P_old(x)
```

古いサンプルを新しいモデルでも使い回すための「重み」。

- P_new > P_old → ratio > 1 → 重視
- P_new < P_old → ratio < 1 → 軽視

### クリッピング

ratio が極端（例: 10倍）だと不安定。[0.8, 1.2] に制限。

**理解した表現**:
> 学習前から学習後に、ratio というサンプルごとに動的な学習率のようなファクタを計算して、出現確率が下がったもののデータはその分軽視して、出現確率が上がったものは重視するための仕組み。

---

## DPO（簡略化手法）

報酬モデル不要。人間が選んだペア（good/bad）から直接学習。

```python
loss = -log(sigmoid(β * (log_prob(chosen) - log_prob(rejected))))
```

普通の fine-tuning に近い形で実装可能。

---

## GRPO（DeepSeek が使用）

複数の出力を生成し、グループ内で相対評価。Value Network が不要。

---

## 比較表

| 手法 | 報酬モデル | 計算コスト | 特徴 |
|------|-----------|-----------|------|
| REINFORCE | 必要 | 低 | 基本、分散大 |
| PPO | 必要 | 中 | サンプル効率良い |
| DPO | 不要 | 低 | 好みデータから直接 |
| GRPO | 不要/任意 | 中 | 検証可能タスク向け |

---

## 有効だった説明のパターン

### 1. 具体的な数値例

「P_old=0.5, P_new=0.3, ratio=0.6」のような具体値で手計算。

### 2. 「何を解決したいか」から始める

「なぜ ratio が必要か」→「古いサンプルを使い回したいから」

### 3. 誤解の明示的な訂正

```
❌ ランダムに更新して良かったら採用
✅ ランダム生成、確定的な勾配更新
```

### 4. 自分の言葉での言い換え確認

「〜という理解で合ってる？」に対して正誤を明確に返す。

---

## 残った疑問・次に調べること

- Nex-N1 の具体的な RL 手法（Tech Report に詳細あり）
- GRPO の崩壊問題（今日の papers にあった）
- エントロピーボーナスの詳細
